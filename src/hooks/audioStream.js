import { useEffect } from "react";
import { useAIStore } from "../store/aiState";

export function useAudioStream(flowType) {
  useEffect(() => {
    console.log("ğŸ¬ useAudioStream initialized with flow:", flowType);
    console.log(" Web socket URL:", process.env.NEXT_PUBLIC_BACKEND_WS);

    const socket = new WebSocket(`${process.env.NEXT_PUBLIC_BACKEND_WS}/ws`);
    socket.binaryType = "arraybuffer";

    let mediaStream;
    let mediaRecorder;

    // Step 1: Setup microphone access (but DO NOT start recording yet)
    navigator.mediaDevices
      .getUserMedia({ audio: true })
      .then((stream) => {
        console.log("ğŸ™ï¸ Microphone access granted");
        mediaStream = stream;

        if (!MediaRecorder.isTypeSupported("audio/webm;codecs=opus")) {
          console.warn("âš ï¸ Opus format not supported in this browser");
        }

        mediaRecorder = new MediaRecorder(stream, {
          mimeType: "audio/webm;codecs=opus",
        });

        mediaRecorder.ondataavailable = (e) => {
          if (e.data.size > 0) {
            e.data.arrayBuffer().then((buffer) => {
              if (socket.readyState === WebSocket.OPEN) {
                socket.send(buffer);
                console.log("ğŸ“¤ Sent audio chunk:", buffer.byteLength, "bytes");
              } else {
                console.warn("ğŸš« Skipped chunk: socket not open");
              }
            });
          }
        };

        mediaRecorder.onerror = (e) => {
          console.error("âŒ MediaRecorder error:", e.error);
        };

        // MediaRecorder will be started later in socket.onopen
      })
      .catch((err) => {
        console.error("âŒ Failed to get microphone access:", err);
      });

    // Step 2: WebSocket events
    socket.onopen = () => {
      console.log("âœ… WebSocket connected");

      const initMessage = { type: "start", flow: flowType };
      socket.send(JSON.stringify(initMessage));
      console.log("ğŸ“¤ Sent init message:", initMessage);

      useAIStore.getState().setAIState("isListening", true);

      // Start audio recording ONLY once socket is ready
      if (mediaRecorder && mediaRecorder.state !== "recording") {
        try {
          mediaRecorder.start(250);
          console.log("ğŸ›ï¸ MediaRecorder started at 250ms intervals");
        } catch (err) {
          console.error("âŒ Failed to start MediaRecorder:", err);
        }
      }
    };

    socket.onmessage = (event) => {
      const data = event.data;
      if (typeof data === "string") {
        const msg = JSON.parse(data);
        console.log("ğŸ“© WS message:", msg);

        if (msg.type === "partial_transcript") {
          useAIStore.getState().setTranscript(msg.text);
          console.log("ğŸ“ Partial:", msg.text);
        } else if (msg.type === "final_transcript") {
          console.log("âœ… Final transcript:", msg.text);
          useAIStore.getState().addMessage("user", msg.text);
          useAIStore.getState().setAIState("isListening", false);
          useAIStore.getState().setAIState("isThinking", true);
        } else if (msg.type === "assistant_response") {
          console.log("ğŸ¤– Assistant:", msg.text);
          useAIStore.getState().addMessage("assistant", msg.text);
          useAIStore.getState().setAIState("isThinking", false);
          if (msg.audio) {
            playAudio(msg.audio);
          }
        }
      } else {
        console.log("ğŸ“¥ Binary audio from backend");
        playAudio(data);
      }
    };

    socket.onerror = (err) => {
      console.error("âŒ WebSocket error:", err);
    };

    socket.onclose = () => {
      console.warn("ğŸ”Œ WebSocket closed");
    };

    return () => {
      console.log("ğŸ§¹ Cleaning up audio stream");
      useAIStore.getState().setAIState("isListening", false);
      try {
        mediaRecorder?.stop();
        mediaStream?.getTracks().forEach((t) => t.stop());
        socket.close();
      } catch (err) {
        console.error("âŒ Cleanup error:", err);
      }
    };
  }, [flowType]);
}

// ğŸ”Š Helper: play audio base64 or ArrayBuffer
function playAudio(audioData) {
  let audioBlob;
  try {
    if (typeof audioData === "string") {
      const binary = atob(audioData);
      const buffer = new Uint8Array(binary.length);
      for (let i = 0; i < binary.length; i++) buffer[i] = binary.charCodeAt(i);
      audioBlob = new Blob([buffer], { type: "audio/mp3" });
    } else {
      audioBlob = new Blob([audioData], { type: "audio/mp3" });
    }

    const audioUrl = URL.createObjectURL(audioBlob);
    const audio = new Audio(audioUrl);
    audio
      .play()
      .then(() => console.log("â–¶ï¸ Audio playing"))
      .catch((err) => console.error("âŒ Audio playback error:", err));

    audio.onended = () => {
      console.log("ğŸ” Audio finished, resuming listening");
      useAIStore.getState().setAIState("isListening", true);
      useAIStore.getState().setTranscript("");
    };
  } catch (err) {
    console.error("âŒ playAudio() error:", err);
  }
}
